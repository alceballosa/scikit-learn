"""
==========================
GMM Initialization Methods
==========================

Examples of the different methods of initialization in Gaussian Mixture Models

See :ref:`gmm` for more information on the estimator.

Here we generate some sample data with four easy to identify clusters. The
purpose of this example is to show the four different methods for the
initialization parameter *init_param*.

- *kmeans* - performs kmeans clustering on the dataset to produce the initial
  centers. This will be the most resource intensive initialization but will
  often produce intitial means that are very close to the converged gmm model.
- *k-means++* - uses the initialization of kmeans clustering (called k-means++)
  to use input data points as the initial centers but to pick them such that
  they are sufficiently far apart from each other.
- *rand_data* - uses randomly selected data points as the initial centers. This
  is a fast method but can potentially lead to unstable results if all of the
  selected points are close together.
- *random* - Is the simplest method. It uses centers that are close to the mean
  of the data. This can be the slowest initialization to converge.

Orange diamonds represent the initialization centers for the gmm generated by
the *init_param*. The rest of the data is represented as crosses and the
colouring represents the eventual associated classification after the GMM has
finished.

The numbers in the top right of each subplot represent the number of
iterations taken for the GaussianMixture to converge and the relative time
taken for the initialization part of the algorithm to run. The shorter
initialization times tend to have a greater number of iterations to converge.

The initialization time is the ratio of the time taken for that method versus
the time taken for the *random* method. As you can see *random* and *rand_data*
are on similar orders of time taken but in general *k-means++* will be slower
and *kmeans* slower still.

In this example, when initialized with *rand_data* or *random* the model takes
more iterations to converge.
"""


# Author Gordon Walsh (github: g-walsh) gordon.p.walsh@gmail.com
# data generation code from https://jakevdp.github.io/

import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.utils.extmath import row_norms
from sklearn.cluster.k_means_ import _k_init
from sklearn.datasets.samples_generator import make_blobs
from timeit import default_timer as timer

print(__doc__)

# Generate some data

X, y_true = make_blobs(n_samples=400, centers=4,
                       cluster_std=0.60, random_state=0)
X = X[:, ::-1]

n_samples = 400
n_components = 4
x_squared_norms = row_norms(X, squared=True)


def gen_gmm(init_params, seed=None):
    r = np.random.RandomState(seed)
    start = timer()
    if init_params == 'kmeans':
        init_means = calculate_means(kmeans_mean(r))
    elif init_params == 'random':
        init_means = calculate_means(rand_mean(r))
    elif init_params == 'rand_data':
        init_means = calculate_means(rand_point_mean(r))
    elif init_params == 'k-means++':
        init_means = calculate_means(kmeanspp_mean(r))
    else:
        raise ValueError("Unimplemented initialisation method '%s'"
                         % init_params)
    end = timer()
    init_time = end - start

    gmm = GaussianMixture(n_components=4, means_init=init_means, tol=1e-9,
                          max_iter=2000, random_state=r).fit(X)

    labels = gmm.predict(X)
    iterations = gmm.n_iter_
    return labels, init_means, seed, init_params, init_time, iterations


def kmeans_mean(r):
    # Calculate the responsibilities by kmeans
    # This will label all data points with one of the components absolutely.
    resp_km = np.zeros((n_samples, n_components))
    label = KMeans(n_clusters=n_components,
                   n_init=1, random_state=r).fit(X).labels_
    resp_km[np.arange(n_samples), label] = 1
    return resp_km


def rand_point_mean(r):
    # Generate responsibilities to pick random points from the data.
    # This will label one random data point for each component. All others 0.
    resp_select_point = np.zeros((n_samples, n_components))
    points = r.choice(range(n_samples), n_components, replace=False)
    for n, i in enumerate(points):
        resp_select_point[i, n] = 1
    return resp_select_point


def rand_mean(r):
    # Generate random responsibilities for all points.
    # This will label all points with random weighting.
    # Sum of responsibilities across a given point is 1.
    resp_random_orig = r.rand(n_samples, n_components)
    resp_random_orig /= resp_random_orig.sum(axis=1)[:, np.newaxis]
    return resp_random_orig


def kmeanspp_mean(r):
    # Generate responsibilities that end up picking points based on k-means++
    resp_kmpp = np.zeros((n_samples, n_components))
    centers, indices = _k_init(X, n_components,
                               x_squared_norms=x_squared_norms, random_state=r)
    for n, i in enumerate(indices.astype(int)):
        resp_kmpp[i, n] = 1
    return resp_kmpp


def calculate_means(resp):
    # Generate the means of the components. These are the initial parameters.
    nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps
    means = np.dot(resp.T, X) / nk[:, np.newaxis]
    return means


# Example plot


def test_seed(seed):
    methods = ['random', 'kmeans', 'rand_data', 'k-means++']
    colors = ['navy', 'turquoise', 'cornflowerblue', 'darkorange']
    times_init = {}
    relative_times = {}

    plt.figure(figsize=(3 * len(methods) // 2, 6))
    plt.subplots_adjust(bottom=.1, top=0.9, hspace=.15, wspace=.05,
                        left=.05, right=.95)

    for n, method in enumerate(methods):
        plt.subplot(2, len(methods) // 2, n+1)
        labels, ini, seed, params, init_time, iters = gen_gmm(method, seed)
        times_init[method] = init_time
        for i, color in enumerate(colors):
            data = X[labels == i]
            plt.scatter(data[:, 0], data[:, 1], color=color, marker='x')

        plt.scatter(ini[:, 0], ini[:, 1], s=75, marker='D', c='orange',
                    lw=1.5, edgecolors='black')
        relative_times[method] = times_init[method] / times_init[methods[0]]

        plt.xticks(())
        plt.yticks(())
        plt.title(method, loc='left')
        plt.title("Iter %i | Init Time %.2fx"
                  % (iters, relative_times[method]), loc='right', fontsize=10)
    plt.suptitle('Gmm iterations and relative time taken to initialize')
    plt.show()


test_seed(1234)
